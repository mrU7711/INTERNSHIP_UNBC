{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53324847",
   "metadata": {},
   "source": [
    "# Notebook 3: MediaPipe + RealSense Depth-Based Gaze Estimation (Real-Time)\n",
    "This notebook uses MediaPipe to detect facial and iris landmarks and combines them with RealSense depth data to estimate a 3D gaze vector in real time.\n",
    "Ensure you have MediaPipe installed (`pip install mediapipe`) and the Intel RealSense SDK set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c96f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                   max_num_faces=1,\n",
    "                                   refine_landmarks=True,\n",
    "                                   min_detection_confidence=0.5,\n",
    "                                   min_tracking_confidence=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e93f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "# Get intrinsics for depth-to-3D conversion\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "intrinsics = profile.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc162a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deproject(x, y, depth):\n",
    "    return rs.rs2_deproject_pixel_to_point(intrinsics, [x, y], depth)\n",
    "\n",
    "print(\"Press 'q' to quit.\")\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        aligned_frames = align.process(frames)\n",
    "        depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        rgb_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb_image)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            h, w, _ = color_image.shape\n",
    "\n",
    "            # Iris center (landmark 468 for left eye)\n",
    "            lm = face_landmarks.landmark[468]\n",
    "            x_px = int(lm.x * w)\n",
    "            y_px = int(lm.y * h)\n",
    "            z = depth_frame.get_distance(x_px, y_px)\n",
    "\n",
    "            if z > 0:\n",
    "                iris_3D = np.array(deproject(x_px, y_px, z))\n",
    "\n",
    "                # Use another point on the eye (e.g., landmark 474) to estimate direction\n",
    "                ref_lm = face_landmarks.landmark[474]\n",
    "                ref_x = int(ref_lm.x * w)\n",
    "                ref_y = int(ref_lm.y * h)\n",
    "                ref_z = depth_frame.get_distance(ref_x, ref_y)\n",
    "\n",
    "                if ref_z > 0:\n",
    "                    ref_3D = np.array(deproject(ref_x, ref_y, ref_z))\n",
    "                    gaze_vector = iris_3D - ref_3D\n",
    "                    gaze_vector /= np.linalg.norm(gaze_vector)\n",
    "\n",
    "                    # Visualize 2D projection of 3D gaze\n",
    "                    pt1 = (x_px, y_px)\n",
    "                    pt2 = (int(x_px + gaze_vector[0] * 100), int(y_px + gaze_vector[1] * 100))\n",
    "                    cv2.arrowedLine(color_image, pt1, pt2, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"MediaPipe + RealSense Gaze\", color_image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
